{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "07.íšŒê·€ë¶„ì„.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FV9i74bDJo2R"
      },
      "source": [
        "# ë‹¨ìˆœ ì„ í˜• íšŒê·€\n",
        "- íšŒê·€ì‹ : y = ğ‘Šğ‘¥+ğ‘ïƒ¼\n",
        "- ì—ëŸ¬ : $$e_i = y_i - \\hat{y_i} = y_i - Wxi -b$$\n",
        "$$(\\hat{y_i} = Wx_i + b)$$\n",
        "- Cost Function : ìµœì†Œì œê³±ë²•(mean square)<br>\n",
        "$$E(W,b) = \\sum_{i=1}^{n} (y_i - Wx_i - b)^{2}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrOAWUv55PWw"
      },
      "source": [
        "# Linear Regression  y= Wx + b\n",
        "import random\n",
        "\n",
        "#Gradient\n",
        "def g_W(x, y, iters):\n",
        "    total=0.0\n",
        "    for i in range(len(x)):\n",
        "        total += W[iters]*pow(x[i],2)+b[iters]*x[i]-x[i]*y[i]\n",
        "    return total\n",
        "\n",
        "def g_b(x, y, iters):\n",
        "    total=0.0\n",
        "    for i in range(len(x)):\n",
        "        total += W[iters]*x[i] + b[iters] - y[i]\n",
        "    return total\n",
        "\n",
        "# cost function\n",
        "def  costfunction(x, y, W, b, iters):\n",
        "    total = 0.0\n",
        "    for i in range(len(x)):\n",
        "        total += pow(W[iters]*x[i]+b[iters]-y[i],2)\n",
        "    return total/2\n",
        "\n",
        "#í…ŒìŠ¤íŠ¸ ì„¸íŠ¸\n",
        "X = [-3, -2, -1, 0, 1, 2, 3]\n",
        "y = [-3, -2, -1, 0, 1, 2, 3]  #y1\n",
        "#y = [-2, -1, -0, 1, 2, 3, 4] #y2\n",
        "# í™”í•™ ë°˜ì‘ \n",
        "#X=[10, 20, 30, 40]\n",
        "#y=[71, 45, 24, 8]\n",
        "# ë™ì•„ë¦¬ í‚¤-ëª¸ë¬´ê²Œ ê´€ê³„ \n",
        "#X=[157, 160, 160, 168, 172, 175, 175, 177, 182, 184, 188, 190]\n",
        "#y=[42,   48,  54,  58,  63,  69,  71,  73,  70,  80, 79, 81]\n",
        "\n",
        "M1_W=[0.0]\n",
        "M1_b=[0.0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "id": "hbG2HI1a5YRv",
        "outputId": "0fe085d5-5ab5-429b-c01e-9688b2a8a47c"
      },
      "source": [
        "# Method 1: statistical\n",
        "print('====================================================')\n",
        "print(' Method 1:  Statistical Method')\n",
        "print('====================================================')\n",
        "\n",
        "Xmean = sum(X)/len(X)\n",
        "Ymean = sum(y)/len(y) \n",
        "print('Xmean:', Xmean, 'Ymean:',Ymean)\n",
        "print('-----------------------------------------------------')\n",
        "total1 = 0\n",
        "total2 = 0\n",
        "for i in range(len(X)):\n",
        "    total1 +=  (y[i]-Ymean)*(X[i]-Xmean)\n",
        "    total2 += pow(X[i]-Xmean, 2)\n",
        "M1_W[0] = total1/total2\n",
        "M1_b[0] = Ymean-M1_W[0]*Xmean\n",
        "\n",
        "print('Linear Regression by Method 1 : y =', M1_W[0],'* x +',M1_b[0])\n",
        "print('-----------------------------------------------------')\n",
        "print('y        y_hat ')\n",
        "print('-------------------')\n",
        "for i in range(len(X)):\n",
        "    print(y[i], '   ', M1_W[0]*X[i]+M1_b[0])\n",
        "print('-------------------')\n",
        "M1_cost = costfunction(X, y, M1_W, M1_b, 0)\n",
        "print('cost: ', M1_cost)\n",
        "print('-------------------')\n",
        "input('strike any key..')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "====================================================\n",
            " Method 1:  Statistical Method\n",
            "====================================================\n",
            "Xmean: 0.0 Ymean: 0.0\n",
            "-----------------------------------------------------\n",
            "Linear Regression by Method 1 : y = 1.0 * x + 0.0\n",
            "-----------------------------------------------------\n",
            "y        y_hat \n",
            "-------------------\n",
            "-3     -3.0\n",
            "-2     -2.0\n",
            "-1     -1.0\n",
            "0     0.0\n",
            "1     1.0\n",
            "2     2.0\n",
            "3     3.0\n",
            "-------------------\n",
            "cost:  0.0\n",
            "-------------------\n",
            "strike any key..q\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'q'"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3DK1Ym-5cBn",
        "outputId": "9a984a25-1f23-4a50-cc52-d08449e9b276"
      },
      "source": [
        "# Method 2\n",
        "print('====================================================')\n",
        "print(' Method 2:  Gradient Descent Method')\n",
        "print('====================================================')\n",
        "W=[0.0]\n",
        "b=[0.0]\n",
        "W[0] = float( random.randint(-100, 100))   ## M1_W[0] #\n",
        "b[0] = float(random.randint(-100, 100))    ##  M1_b[0] #\n",
        "\n",
        " #This tells us when to stop the algorithm\n",
        "iters = 0 #iteration counter\n",
        "cost = costfunction(X, y, W, b, iters) \n",
        "\n",
        "print(\"Iteration\",iters,\"\\tW[0]:\",W[0],\"\\tb[0]:\",b[0],\"\\tcost:\",cost)\n",
        "\n",
        "rate =  10/cost # Learning rate   ## 0.000001* cost #\n",
        "MaxItrs = cost   ## 10000  #\n",
        "precision = 0.0001       ## M1_cost *0.8\n",
        "while cost > precision: # and iters < MaxItrs:\n",
        "    iters = iters+1\n",
        "    gradientW = g_W(X, y, iters-1)\n",
        "    gradientB = g_b(X, y, iters-1)\n",
        "    newW = W[iters-1] - rate*gradientW\n",
        "    newb = b[iters-1] - rate*gradientB\n",
        "    W.append(newW)\n",
        "    b.append(newb)\n",
        "    if iters %100==0:  # ë™ì•„ë¦¬ í‚¤-ëª¸ë¬´ê²Œ ì‚¬ë¡€ì—ì„œëŠ” 1000000ë¡œ ì„¤ì •\n",
        "        print('iteration: ', iters, end=',')\n",
        "        print('gradient W: %.1f, gradient b:%.1f, ' % (gradientW, gradientB), end=' ')\n",
        "        print('W[ %d ]:%.1f, b[%d]:%.1f, cost: %.1f'\n",
        "                % (iters, W[iters] , iters, b[iters], cost))\n",
        "        ans = input()\n",
        "        if ans =='q':           \n",
        "            break\n",
        "print('iteration: ', iters, end=',')\n",
        "print('gradient W: %.1f, gradient b:%.1f, ' % (gradientW, gradientB), end=' ')\n",
        "print('W[ %d ]:%.1f, b[%d]:%.1f, cost: %.1f'\n",
        "                % (iters, W[iters] , iters, b[iters], cost))\n",
        "print('-----------------------------------------------------------')\n",
        "print('Linear Regression by Method2: y =', W[iters],'* x +',b[iters])\n",
        "print('-----------------------------------------------------------')\n",
        "print('y        y_hat ')\n",
        "print('---------------------')\n",
        "for i in range(len(X)):\n",
        "    print(y[i], '   ', W[iters]*X[i]+b[iters])\n",
        "\n",
        "\n",
        "print('====================================================')\n",
        "print('%10s   %10s   %10s'%  ('y','M1:y_hat','M2:y_hat'))\n",
        "print('-----------------------------------------------------')\n",
        "for i in range(len(X)):\n",
        "    print('%10.1f   %10.3f   %10.3f' % (float(y[i]), M1_W[0]*X[i]+M1_b[0],  W[iters]*X[i]+b[iters]) )\n",
        "print('------------------------------------------------------')\n",
        "print('%10s:  %10.3f   %10.3f' % ('cost', costfunction(X, y, M1_W, M1_b, 0), costfunction(X, y, W, b, iters)) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "====================================================\n",
            " Method 2:  Gradient Descent Method\n",
            "====================================================\n",
            "Iteration 0 \tW[0]: -76.0 \tb[0]: -75.0 \tcost: 102693.5\n",
            "iteration:  100,gradient W: -1645.4, gradient b:-490.7,  W[ 100 ]:-57.6, b[100]:-70.1, cost: 102693.5\n",
            "q\n",
            "iteration:  100,gradient W: -1645.4, gradient b:-490.7,  W[ 100 ]:-57.6, b[100]:-70.1, cost: 102693.5\n",
            "-----------------------------------------------------------\n",
            "Linear Regression by Method2: y = -57.6024806708906 * x + -70.05641658976289\n",
            "-----------------------------------------------------------\n",
            "y        y_hat \n",
            "---------------------\n",
            "-3     102.75102542290891\n",
            "-2     45.14854475201831\n",
            "-1     -12.45393591887229\n",
            "0     -70.05641658976289\n",
            "1     -127.65889726065349\n",
            "2     -185.26137793154408\n",
            "3     -242.86385860243468\n",
            "====================================================\n",
            "         y     M1:y_hat     M2:y_hat\n",
            "-----------------------------------------------------\n",
            "      -3.0       -3.000      102.751\n",
            "      -2.0       -2.000       45.149\n",
            "      -1.0       -1.000      -12.454\n",
            "       0.0        0.000      -70.056\n",
            "       1.0        1.000     -127.659\n",
            "       2.0        2.000     -185.261\n",
            "       3.0        3.000     -242.864\n",
            "------------------------------------------------------\n",
            "      cost:       0.000    65257.166\n"
          ]
        }
      ]
    }
  ]
}